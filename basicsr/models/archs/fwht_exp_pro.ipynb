{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a02595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from basicsr.models.archs.arch_util import LayerNorm2d\n",
    "from basicsr.models.archs.local_arch import Local_Base\n",
    "# from torchinfo import summary\n",
    "import numpy as np\n",
    "# from scipy.linalg import hadamard\n",
    "from hadamard_transform import hadamard_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_power(x, p=2):\n",
    "    y = 1\n",
    "    while y<x:\n",
    "        y *= p\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftThresholding(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.T = torch.nn.Parameter(torch.rand(self.num_features)/10)\n",
    "              \n",
    "    def forward(self, x):\n",
    "        print(x.shape,self.T.shape)\n",
    "#         return torch.mul(torch.sign(x), torch.nn.functional.relu(torch.abs(x)-torch.abs(self.T)))\n",
    "        return torch.mul(torch.tanh(x), torch.nn.functional.relu(torch.abs(x)-torch.abs(self.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hadamard_transform(u, axis=-1, fast=True):\n",
    "    \"\"\"Multiply H_n @ u where H_n is the Hadamard matrix of dimension n x n.\n",
    "    n must be a power of 2.\n",
    "    Parameters:\n",
    "        u: Tensor of shape (..., n)\n",
    "        normalize: if True, divide the result by 2^{m/2} where m = log_2(n).\n",
    "    Returns:\n",
    "        product: Tensor of shape (..., n)\n",
    "    \"\"\"  \n",
    "    if axis != -1:\n",
    "        u = torch.transpose(u, -1, axis)\n",
    "    \n",
    "    n = u.shape[-1]\n",
    "    m = int(np.log2(n))\n",
    "    assert n == 1 << m, 'n must be a power of 2'\n",
    "    if fast:\n",
    "        x = u[..., np.newaxis]\n",
    "        for d in range(m)[::-1]:\n",
    "            x = torch.cat((x[..., ::2, :] + x[..., 1::2, :], x[..., ::2, :] - x[..., 1::2, :]), dim=-1)\n",
    "        y = x.squeeze(-2) / 2**(m / 2)\n",
    "    else:\n",
    "        H = torch.tensor(hadamard(n), dtype=torch.float, device=u.device)\n",
    "        y = u @ H.t()/np.sqrt(n)\n",
    "    if axis != -1:\n",
    "        y = torch.transpose(y, -1, axis)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a25f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WHT_expansion(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    通道数扩展\n",
    "    num_features: Length of the last axis, should be interger power of 2. If not, we pad 0s.\n",
    "    residual: Apply shortcut connection or not\n",
    "    retain_DC: Retain DC channel (the first channel) or not\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features , output_features , residual=False , retain_DC=True):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.num_features_pad = find_min_power(self.output_features)  \n",
    "        self.ST = SoftThresholding(self.num_features_pad)    \n",
    "        self.residual = residual\n",
    "        self.retain_DC = retain_DC\n",
    "\n",
    "         \n",
    "    def forward(self, x):\n",
    "        input_features = x.shape[-1]\n",
    "        if input_features!= self.input_features:\n",
    "            raise Exception('{}!={}'.format(input_features, self.input_features))\n",
    "        if self.num_features_pad>input_features:\n",
    "            f0 = torch.nn.functional.pad(x, (0, self.num_features_pad-input_features))\n",
    "        else:\n",
    "            f0 = x\n",
    "        f1 = hadamard_transform(f0)\n",
    "\n",
    "#         f2 = self.v*f1\n",
    "        f3 = self.ST(f1)\n",
    "        # 如果需要，添加直流分量\n",
    "        if self.retain_DC:\n",
    "            f3[..., 0] = f1[..., 0]  # 恢复直流分量\n",
    "        f4 = hadamard_transform(f3)\n",
    "        y = f4[..., :self.output_features]\n",
    "        if self.residual:\n",
    "            y = y + x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c389d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WHT_projection(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    通道数减少 \n",
    "    num_features: Length of the last axis, should be interger power of 2. If not, we pad 0s.\n",
    "    residual: Apply shortcut connection or not\n",
    "    retain_DC: Retain DC channel (the first channel) or not\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features , output_features , residual=False , retain_DC=True):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.input_features_pad = find_min_power(self.input_features)  \n",
    "        self.output_features_pad = find_min_power(self.output_features)  \n",
    "        self.r = int(2**(self.input_features_pad - self.output_features_pad))\n",
    "        self.ST = SoftThresholding(int(2**self.input_features_pad - self.r + 1))    \n",
    "        \n",
    "        self.residual = residual\n",
    "        self.retain_DC = retain_DC\n",
    "         \n",
    "    def forward(self, x):\n",
    "        input_features = x.shape[-1]\n",
    "        if input_features!= self.input_features:\n",
    "            raise Exception('{}!={}'.format(input_features, self.input_features))\n",
    "        if self.input_features_pad>input_features:\n",
    "            f0 = torch.nn.functional.pad(x, (0, self.input_features_pad-input_features))\n",
    "        else:\n",
    "            f0 = x\n",
    "        f1 = hadamard_transform(f0)\n",
    "            \n",
    "        # 计算要平均池化的通道范围\n",
    "        start_channel = 1  # 从通道1开始\n",
    "        end_channel = int(2 ** self.input_features_pad - self.r + 1)  # 计算结束通道\n",
    "        print(end_channel)\n",
    "        # 选择要进行平均池化的通道\n",
    "        f2 = f1[:, :, :, start_channel:end_channel+1]\n",
    "#         print(f2.shape)\n",
    "        # 对选定通道进行平均池化\n",
    "        n = f2.shape[0]\n",
    "        f3 = []\n",
    "        for i in range(n):\n",
    "            f3.append(F.avg_pool1d(f2[i], kernel_size=self.r, stride=self.r , padding=0))\n",
    "        f3 = torch.stack(f3)\n",
    "        f4 = torch.cat(f1[:,:,:,0]/self.r , f3 , dim=-1)\n",
    "        f5 = hadamard_transform(f4)\n",
    "        y = f4[..., :self.output_features]\n",
    "        if self.residual:\n",
    "            y = y + x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "img=torch.arange(8*4*4).reshape(1,8,4,4)\n",
    "img_t = img.permute(0,3,2,1)\n",
    "# # 池化核和池化步长均为2\n",
    "pool=nn.AvgPool1d(2,stride=2)\n",
    "img_2=pool(img_t[0])\n",
    "n = img_t.shape[0]\n",
    "temp = []\n",
    "for i in range(n):\n",
    "    temp.append(F.avg_pool1d(img_t[i], kernel_size=2 , stride=2))\n",
    "temp = torch.stack(temp)\n",
    "\n",
    "print(img,img.shape)\n",
    "print(img_t,img_t.shape)\n",
    "print(img_2,img_2.shape)\n",
    "print(temp , temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39992013",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = WHT_expansion(input_features=4 , output_features=8)\n",
    "ex_ten = net(img_t)\n",
    "print(ex_ten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac8cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = WHT_projection(input_features=8 , output_features=2)\n",
    "ex_ten1 = net1(img_t)\n",
    "print(ex_ten1.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
